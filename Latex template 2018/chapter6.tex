\chapter{جمع‌بندي و نتيجه‌گيري و پیشنهادات}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{نتیجه‌گیری}
این پایان‌نامه موفق به انجام مسیریابی و کنترل اجماع به طور مستقل و در نهایت ادغام این دو با یکدیگر بوده است. همچنین از مباحث گسترده‌ای همچون کنترل دیجیتال، کنترل مدرن، رباتیک پیشرفته، جبرخطی، نظریه گراف، طراحی الگوریتم، هوش مصنوعی و یادگیری تقویتی در این پروژه بهره برده شده است. همچنین علاوه بر نتایج موفق به خصوص در حوزه \lr{Q-learning} و الگوریتم‌های ابتکاری، در بحث تئوری نیز پایداری الگوریتم مطرح شده اثبات گردیده است. در ادامه جا دارد که نکات مهم این پایان‌نامه مرور گردند:
\begin{enumerate}
	\item چالش محاسبه سینماتیک ربات که با استفاده از مدل ساده‌تر به دست آمد.
	\item چالش برطرف کردن کنترل زاویه هنگام ناپیوستگی بین $\pi$ و $-\pi$ که با پیدا کردن کمترین اختلاف در هر دو جهت عقربه‌های ساعت و خلاف عقربه‌های ساعت به دست آمد.
	\item در روش پیشنهادی برای کنترل اجماع در این پایان‌نامه دیگر نیازی به اینکه همانند پایان‌نامه‌های گذشته، مجموع درایه‌های روی ماتریس شکل‌دهی برابر با صفر باشند نبود و همچنین ربات‌ها دقیقا به مکان خواسته شده می‌رفتند در حالی که در روش پیشنهادی گذشته، قرارگیری ربات‌ها علاوه بر ماترس $F$ به ماتریس $L$ نیز وابسته بود. همچنین پایداری این روش اثبات گردید و با یک حقه قرارگیری در مختصات \lr{x-y} را نیز نسبت به هم مستقل کردیم تا راحت‌تر بتوان ماتریس $F$ مورد نظر را تشکیل داد.
	\item مشکل تاخیر پاسخ توسط عامل‌ها نسبت به عامل رهبر که با مجازی در نظر گرفتن رهبر حل گردید.
	\item کد بسیار مرتب پیاده‌سازی شده به طوری که با ساختن نقشه‌های جدید تنها با وارد کردن مختصات موانع یا تغییر در ابعاد نقشه، فایل‌ها قابل اجرا باشند.
	\item یکی از چالش‌های مهم، نحوه تعریف محیط پیوسته، شامل حالت‌ها و اقدامات ممکن، برای الگوریتم‌های مسیریابی بود.
	\item پیاده‌سازی الگوریتم‌های ابتکاری و \lr{Q-learning} و مقایسه آن‌ها یکی از اهداف مهم پایان‌نامه بود.
	\item تنظیم پارامتر‌های الگوریتم \lr{Q-learning} و به خصوص تابع پاداش آن بسیار پیچیده و دشوار بود که با بررسی حالت‌های مختلف و به خصوص اشتباهات رایج، توانستیم با زمان مناسب به پاسخ اغلب بهین دست یابیم.
	\item در نهایت نیز، پیاده‌سازی الگوریتم‌های مسیریابی بر روی دسته ربات‌ها در حضور موانع ثابت و متحرک، و همچنین تخمین مکان ربات‌های خارج دسته، نتیجه نهایی این پایان‌نامه بود.

\end{enumerate}
\section{پیشنهادات}
در آینده می‌توان در این زمینه‌ها فعالیت نمود:
\begin{itemize}
	\item 
	در ابتدا می‌توان سعی بر کنترل دینامیکی ربات‌ها و یا خودرو به صورت تکی نمود. همچنین از کنترل پیشبین مدل\LTRfootnote{model predictive control} برای کنترل تک ربات یا حتی اجماع می‌توان بهره برد.
	\item 
	در الگوریتم اجماع از روش تک انتگرال‌گیر استفاده شده است. می‌توان از روش‌های دو انتگرال‌گیری برای کاهش تاخیر پاسخ بقیه ربات‌ها نسبت به ربات رهبر بهره برد. همچنین روش‌هایی همانند تخمین موقعیت ربات رهبر، و سعی به رسیدن به موقعیت آن در چند گام بعدتر نیز تلاش نمود.
	
	\item 
	در خصوص توضیحات داده شده در مورد گراف‌ها می‌توان الگوریتم‌های مناسبی برای تولید گراف اجماع به طوری که نسبت به قطع ارتباط مقاوم باشند دست یافت.
	\item 
	می‌توان الگوریتم‌های مسیریابی دیگری همچون $RRT$ و $RRT^*$ را پیاده‌سازی نمود.
	\item 
	در تشکیل محیط و حالات، به جای انتخاب مختصات با اعداد صحیح، به صورت تصادفی نقطه‌دهی کرد و وجود مانع یا نبود آن را بین حالت‌ها بررسی کرد. سپس پاسخ الگوریتم‌های $A^*$، \lr{BFS} و سایر الگوریتم‌ها را بررسی نمود.
	\item 
	در زمینه یادگیری تقویتی، می‌توان از روش‌های دیگر نیز استفاده نمود و نتایج را بررسی کرد تا به بهترین روش دست یابیم.
	\item 
	تابع پاداش‌های متفاوت دیگر را نیز در الگوریتم \lr{Q-learning} بررسی کرد و حتی اهدافی دیگر را نیز علاوه بر کوتاهی مسیر به تابع پاداش اضافه نمود. به عنوان مثال می‌توان به تلاش برای کم کردن تغییر جهت در ربات با جریمه کردن آن به ازای اندازه تغییر در $\theta_{desired}$ یا کاهش تغییرات شتاب وارده\LTRfootnote{jerk} به ربات در مسیر برای راحتی سرنشینان، اشاره نمود.
	\item 
	در آخر نیز می‌توان یادگیری تقویتی را به جای طرح مسیر، بر روی ولتاژ چرخ‌های ربات، اعمال نمود و در نتیجه هوشمندی ربات را افزایش داد. هر چند باید در کنار آن راهی برای امنیت خودرو نسبت به تصادفات نیز پیدا کرد، زیرا در حال حاضر اثبات پایداری برای الگوریتم‌های تقویتی به خصوص در سطح کنترل چرخ‌ها موجود نمی‌باشد.
	
	
	
	
	
	
	
	
	
	
	
	
\end{itemize}